# -*- coding: utf-8 -*-
"""Employee_Rentation_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/MrSapkota/employee_retention_prediction/blob/main/Employee_Rentation_Prediction.ipynb

#Loading Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder

"""#Loading Dataset"""

df_test = pd.read_csv('/content/aug_test.csv')
df_train = pd.read_csv('/content/aug_train.csv')

"""#Data Describe"""

df_test.shape

df_train.shape

df_test.info()
df_train.info()

df_train.info()

df_test.head()

df_train.head()

"""# Categorical Columns"""

categorical_cols = ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']
for col in categorical_cols:
    if col in df_test.columns:
        print(f"\nUnique values in df_test for {col}: {df_test[col].unique()}")
        print(f"Value counts in df_test for {col}:\n{df_test[col].value_counts()}")
    if col in df_train.columns:
        print(f"\nUnique values in df_train for {col}: {df_train[col].unique()}")
        print(f"Value counts in df_train for {col}:\n{df_train[col].value_counts()}")

"""This code segment helps us to  understand the categorical data within the  datasets by identifying the different categories (unique values) and how often they occur (value counts) for each categorical column in both the training and test datasets.

# Numerical Columns
"""

numerical_cols = ['city_development_index', 'training_hours']
print("\ndf_test numerical columns description:")
display(df_test[numerical_cols].describe())
print("\ndf_train numerical columns description:")
display(df_train[numerical_cols].describe())

"""The code selects the numerical columns (city_development_index, training_hours) from both df_test and df_train DataFrames. It then uses the .describe() method to calculate and display descriptive statistics (like mean, standard deviation, min, max, percentiles) for these columns in both datasets, enabling easy comparison of their numerical characteristics.

#Checking the missing Value
"""

df_test.isnull().sum()

df_train.isnull().sum()

"""#Data analysis
Subtask:
Compare and contrast the descriptive statistics of numerical features between the two datasets (df_test and df_train). Identify any significant differences in distributions or potential biases.
"""

numerical_cols = ['city_development_index', 'training_hours']

# Calculating descriptive statistics
test_stats = df_test[numerical_cols].describe()
train_stats = df_train[numerical_cols].describe()

# Display the descriptive statistics for comparison
display(test_stats)
display(train_stats)

# Comparing the statistics and identify potential biases
print("Comparison of descriptive statistics:")
for col in numerical_cols:
    print(f"\nColumn: {col}")
    print(f"  Test Mean: {test_stats.loc['mean', col]:.2f}, Train Mean: {train_stats.loc['mean', col]:.2f}")
    print(f"  Test Median: {test_stats.loc['50%', col]:.2f}, Train Median: {train_stats.loc['50%', col]:.2f}")
    print(f"  Test Std Dev: {test_stats.loc['std', col]:.2f}, Train Std Dev: {train_stats.loc['std', col]:.2f}")

"""The code iterates through each numerical column and prints out a comparison of key statistics:

Mean: The average value of the column.

Median: The middle value of the column when sorted.

Standard Deviation: A measure of how spread out the data is.
"""

# Checking for significant differences (example threshold, adjust as needed)
    if abs(test_stats.loc['mean', col] - train_stats.loc['mean', col]) > 0.1:
        print("  Potential bias detected: Significant difference in means.")

"""It checks for potential biases by comparing the means of the numerical columns between the test and train datasets.
If the absolute difference between the means is greater than 0.1 (a threshold chosen by the analyst), it prints a warning about a potential bias.

This code snippet helps us to understand the characteristics of the numerical features in both datasets and identify any potential differences that might indicate biases or issues that need to be addressed before building a machine learning model.

## Exploratory Data Analysis

### Subtask:
Visualize the distributions of key numerical and categorical features in both datasets to identify potential outliers and patterns.  Using histograms, box plots, and bar charts. Handle missing values by dropping rows with NaN in the relevant columns.
"""

# Numerical Features
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))
fig.suptitle('Distributions of Numerical Features')

# city_development_index
sns.histplot(df_train['city_development_index'].dropna(), ax=axes[0, 0], kde=True, color='skyblue')
axes[0, 0].set_title('city_development_index (Train)')
sns.histplot(df_test['city_development_index'].dropna(), ax=axes[0, 1], kde=True, color='salmon')
axes[0, 1].set_title('city_development_index (Test)')

# training_hours
sns.boxplot(y=df_train['training_hours'].dropna(), ax=axes[1, 0], color='lightgreen', whis=3)
axes[1, 0].set_title('training_hours (Train)')
sns.boxplot(y=df_test['training_hours'].dropna(), ax=axes[1, 1], color='lightcoral', whis=3)
axes[1, 1].set_title('training_hours (Test)')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""city_development_index: The histograms would reveal the distribution of city development indices in both the training and test sets.

Skewness: Whether the distribution is skewed (leaning more towards lower or higher values).

Modality: Whether there are multiple peaks in the distribution, suggesting distinct groups of cities with different development levels.

Differences between Train and Test: the distributions in the training and test sets are similar or significantly different, which could impact model performance.



training_hours: The box plots would provide information about:

Central Tendency: The median (represented by the line inside the box) of training hours.

Spread/Dispersion: The interquartile range (IQR), represented by the box's height, shows the spread of the middle 50% of the data.

Outliers: Points outside the whiskers of the box plot are potential outliers, indicating unusually high or low training hours.

Differences between Train and Test: comparing the medians, IQRs, and outlier patterns between the training and test sets to see if there are significant differences.
"""

# Categorical Features
categorical_cols = ['gender', 'relevent_experience', 'enrolled_university', 'education_level',
                    'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']
num_cols = len(categorical_cols)
fig, axes = plt.subplots(nrows=(num_cols + 2) // 3, ncols=min(3, num_cols), figsize=(15, 5 * ((num_cols + 2) // 3)))
fig.suptitle('Distributions of Categorical Features')

for i, col in enumerate(categorical_cols):
    ax = axes[i // 3, i % 3]
    sns.countplot(x=col, data=df_train.dropna(subset=[col]), ax=ax, color='skyblue')
    ax.set_title(f'{col} (Train)')
    ax.tick_params(axis='x', rotation=45)

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""Gender: If the count plot for 'gender' shows a significant difference between the number of males and females, it could indicate a potential bias in the dataset.

Relevant Experience: The count plot for 'relevent_experience' can reveal the proportion of candidates with prior experience, which might be a crucial factor for predicting their job-seeking behavior.

Education Level: The distribution of education levels can offer insights into the qualifications of candidates in the dataset.

Company Size: The count plot for 'company_size' can highlight the prevalence of different company sizes, which might be relevant for understanding job market trends.
"""

# Target Variable
plt.figure(figsize=(8, 6))
sns.countplot(x='target', data=df_train, palette=['lightcoral', 'skyblue'])
plt.title('Distribution of Target Variable')
plt.show()

"""the target variable plot provides crucial information about the distribution of the target variable and helps identify the presence of class imbalance, which is a critical factor to consider when building a classification model. By carefully examining this plot, we can make informed decisions about addressing class imbalance and improving the performance of our model.

#Handling Missing Values and Outliers
"""

# Define numerical and categorical features
numerical_cols = ['city_development_index', 'training_hours']
categorical_cols = ['gender', 'relevent_experience', 'enrolled_university', 'education_level',
                    'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']
features = numerical_cols + categorical_cols # Combine for easier processing

# Handle missing values using SimpleImputer
imputer_num = SimpleImputer(strategy='mean')
imputer_cat = SimpleImputer(strategy='most_frequent')

df_train[numerical_cols] = imputer_num.fit_transform(df_train[numerical_cols])
df_test[numerical_cols] = imputer_num.transform(df_test[numerical_cols])

df_train[categorical_cols] = imputer_cat.fit_transform(df_train[categorical_cols])
df_test[categorical_cols] = imputer_cat.transform(df_test[categorical_cols])

# Handling Outliers (example using IQR for numerical features)
def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    return data

for numerical_col in numerical_cols:
    df_train = remove_outliers_iqr(df_train, numerical_col)
    df_test = remove_outliers_iqr(df_test, numerical_col)

"""this code snippet prepares the datasets for machine learning by handling missing values using imputation and removing outliers based on the IQR. This ensures that the data is more robust and suitable for training and evaluating models."""

# Create box plots for numerical features to check the outliers again
for numerical_col in numerical_cols:
    plt.figure(figsize=(8, 6))  # Adjust figure size as needed
    sns.boxplot(y=df_train[numerical_col], color='skyblue', whis=2.5)  # Adjust whisker length as needed
    plt.title(f'Box Plot of {numerical_col} (After Outlier Removal)')
    plt.show()

for numerical_col in numerical_cols:
    plt.figure(figsize=(8, 6))  # Adjust figure size as needed
    sns.histplot(df_train[numerical_col], kde=True, color='skyblue')
    plt.title(f'Histogram of {numerical_col} (After Outlier Removal)')
    plt.show()

"""By using these plotting techniques after outlier removal, we can visually assess the impact of the outlier handling process and gain further insights into the data.

#Defining Variables
"""

# Defining target variable
target = 'target'

#Defining numerical and categorical features
numerical_features = ['city_development_index', 'training_hours']
categorical_features = ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level',
                        'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']

# Reseting index after outlier removal
df_train = df_train.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)

# Ensuring df_train and df_test have the expected features
common_features = list(set(numerical_features + categorical_features).intersection(df_train.columns))
features = common_features

"""This part finds common features between the defined numerical and categorical features and those actually present in df_train. It ensures that both training and testing datasets use the same features for model training and prediction.

#Data Splitting
"""

# Spliting the training data
X_train, X_val, y_train, y_val = train_test_split(df_train[features], df_train[target], test_size=0.2, random_state=42)

"""test_size=0.2: Indicates 20% of the data is used for validation.
random_state=42: Ensures the split is reproducible

#Encoding
"""

# One-hot encoding for categorical features
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

# Fitting encoder on training data
X_train_encoded = encoder.fit_transform(X_train[categorical_features])
X_val_encoded = encoder.transform(X_val[categorical_features])
df_test_encoded = encoder.transform(df_test[categorical_features])

# Converting encoded arrays into DataFrames
encoded_feature_names = encoder.get_feature_names_out(categorical_features)
X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_feature_names, index=X_train.index)
X_val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoded_feature_names, index=X_val.index)
df_test_encoded_df = pd.DataFrame(df_test_encoded, columns=encoded_feature_names, index=df_test.index)

"""The code encodes the categorical features in the training, validation, and test sets and then converts the encoded data back into pandas DataFrames for easier handling.

#Data Merging and Preparation
"""

# Keeping only numerical features that exist in the dataset
numerical_features_present = list(set(numerical_features).intersection(df_train.columns))

# Merging encoded features with numerical features
X_train = pd.concat([X_train[numerical_features_present], X_train_encoded_df], axis=1)
X_val = pd.concat([X_val[numerical_features_present], X_val_encoded_df], axis=1)
df_test = pd.concat([df_test[numerical_features_present], df_test_encoded_df], axis=1)

"""It combines the encoded categorical features with the numerical features to create the final datasets for model training and prediction."""

# Ensuring test set has the same features and order as X_train
df_test = df_test.reindex(columns=X_train.columns, fill_value=0)

"""It ensures the test set (df_test) has the same features and order as the training set (X_train), filling any missing columns with 0."""

X_train_np = np.array(X_train)  # Converting training set to NumPy array
df_test_np = np.array(df_test)  # Converting test set to NumPy array

"""the training and test sets are converted into NumPy arrays (X_train_np, df_test_np), which are often the required input format for many machine learning models in scikit-learn.

#Initializing the models
"""

log_reg = LogisticRegression(penalty='l2', C=1.0, solver='liblinear')
rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
knn = KNeighborsClassifier(n_neighbors=5)
gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

"""Logistic Regression finds the best line to do this, and the parameters help control how the line is drawn to avoid it being too specific to the training data and not generalizing well to new data.

Random Forest as a group of experts (decision trees) making predictions. Each expert has a slightly different perspective (based on different subsets of the data), and the final prediction is based on a combination of their votes.

a map with different colored points representing different categories. To classify a new point, you look at the colors of its closest neighbors and assign it the most common color among them.
"""

# Store results
results = {}
models = {"Logistic Regression": log_reg, "Random Forest": rf, "KNN": knn, "Gradient Boosting Classifier":gbm}
f1_results = {}         # Compute F1-score for each model

"""These three lines are setting up dictionaries to store important information about the machine learning models that will be trained and evaluated in the code. These dictionaries will make it easy to.

Track the performance (accuracy and F1-score) of each model.

Access the trained models themselves for later use (e.g., making predictions)

#Training & evaluate Logistic Regression
"""

log_reg.fit(X_train, y_train)

y_pred_log = log_reg.predict(X_val)
results["Logistic Regression"] = accuracy_score(y_val, y_pred_log)
print(f"Logistic Regression Accuracy: {results['Logistic Regression']:.4f}")

""" train a Logistic Regression model, uses it to make predictions on validation data, calculates the accuracy of those predictions, and finally displays the accuracy to the user that is 0.7822"""

f1_results["Logistic Regression"] = f1_score(y_val, y_pred_log, average="weighted")

models["Logistic Regression"] = log_reg
print(f"Logistic Regression - Accuracy: {results['Logistic Regression']:.4f}, F1-Score: {f1_results['Logistic Regression']:.4f}")

"""#Training & evaluate Random Forest"""

rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_val)
results["Random Forest"] = accuracy_score(y_val, y_pred_rf)
print(f"Random Forest Accuracy: {results['Random Forest']:.4f}")

f1_results["Random Forest"] = f1_score(y_val, y_pred_rf, average="weighted")

models["Random Forest"] = rf
print(f"Random Forest - Accuracy: {results['Random Forest']:.4f}, F1-Score: {f1_results['Random Forest']:.4f}")

"""#K-Nearest Neighbors"""

knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_val)
results["KNN"] = accuracy_score(y_val, y_pred_knn)
print(f"KNN Accuracy: {results['KNN']:.4f}")

f1_results["KNN"] = f1_score(y_val, y_pred_knn, average="weighted")

models["KNN"] = knn
print(f"KNN - Accuracy: {results['KNN']:.4f}, F1-Score: {f1_results['KNN']:.4f}")

"""#Training and evaluting Gradient Boosting Classifier"""

gbm.fit(X_train, y_train)

y_pred_gbm = gbm.predict(X_val)

results["Gradient Boosting"] = accuracy_score(y_val, y_pred_gbm)

f1_results["Gradient Boosting"] = f1_score(y_val, y_pred_gbm, average="weighted")

models["Gradient Boosting"] = gbm

print(f"Gradient Boosting - Accuracy: {results['Gradient Boosting']:.4f}, F1-Score: {f1_results['Gradient Boosting']:.4f}")

"""#Selecting the best model on Accuracy"""

best_f1_model_name = max(f1_results, key=f1_results.get)
print(f"\nBest Model based on F1-Score: {best_f1_model_name} with F1-Score: {f1_results[best_f1_model_name]:.4f}")

"""#Making predictions on the test set using the best model"""

best_model = models[best_f1_model_name] # Use best_f1_model_name instead of best_model_name
predictions = best_model.predict(df_test_np)

"""#Converting results to a DataFrame"""

df_results = pd.DataFrame({
    "Model": list(results.keys()),
    "Accuracy": list(results.values()),
    "F1-Score": list(f1_results.values())
})

"""#Ploting model performance"""

plt.figure(figsize=(10, 5))
df_results.set_index("Model").plot(kind="bar", figsize=(10, 5), colormap="viridis")

# Add labels and title
plt.xlabel("Model")
plt.ylabel("Score")
plt.title("Model Comparison: Accuracy & F1-Score")
plt.xticks(rotation=0)

# Explicitly set legend labels
plt.legend(["Accuracy", "F1-Score"], loc="best")
plt.grid(axis="y", linestyle="--")

# Show plot
plt.show()

"""#Saving the predictions"""

df_test["Predicted_Target"] = predictions
df_test[["Predicted_Target"]].to_csv("predictions.csv", index=False)

"""#Downloading the prediction"""

#from google.colab import files
#files.download("predictions.csv")

"""## Summary:

### 1. Q&A

The provided analysis doesn't explicitly pose questions. However, the analysis implicitly seeks to understand the datasets' characteristics, identify potential biases, and explore feature distributions.  Based on the analysis, the `training_hours` feature shows a slight difference in mean between the training and test sets, prompting further investigation to confirm if this difference is practically significant.  The visualizations also implicitly ask us to examine the distributions of features to find patterns and potential outliers.


### 2. Data Analysis Key Findings

* **Dataset Size Discrepancy:** `df_train` (19158 rows, 14 columns) is significantly larger than `df_test` (2129 rows, 13 columns).  The presence of a 'target' column in `df_train` but not in `df_test` indicates it's likely the training set for a prediction task, with `df_test` being the corresponding test set.
* **Missing Values:**  Multiple columns in both datasets contain missing values, with 'gender' and 'company_size' exhibiting the most missing data. This will need to be addressed during preprocessing.
* **Potential Bias in Training Hours:** The mean of 'training_hours' differs slightly between the training (65.37) and test (64.98) sets.  While the difference in means exceeds the chosen threshold (0.1), the medians are identical, requiring further investigation to ascertain the practical significance of this discrepancy.
* **Skewed Target Variable:** The visualization of the target variable reveals an imbalance between the classes (not specified in the provided text). This imbalance should be addressed when training a classification model.



###3. Descriptive Statistics:

* Descriptive statistics for numerical features were calculated and compared between the training and test sets.
* There was a slight difference in the mean of training_hours between the two sets, but the medians were identical. Further investigation is needed to determine if this difference is significant.

###4. Data Visualization:

* Histograms, box plots, and bar charts were used to visualize the distributions of key numerical and categorical features.
* These visualizations revealed potential outliers in training_hours and class imbalance in the target variable.

###5. Potential Biases:

* The potential bias in training_hours needs further investigation.
* The class imbalance in the target variable should be addressed during model training (e.g., using oversampling, undersampling, or cost-sensitive learning).

###6. Outlier Handling:

* Outliers in numerical features (training_hours) were handled using the IQR method.
* Outlier handling is important to prevent them from unduly influencing model training.


###7. Model Selection:

* Four models were selected for this task: Logistic Regression, Random Forest, and K-Nearest Neighbors (KNN), Gradient Boosting Classifier.
* These models were chosen based on their suitability for classification tasks and the nature of the dataset.
Logistic Regression is a simple, interpretable model, while Random Forest and KNN are more complex, non-linear models.
* Including a variety of models allows for comparison and helps identify the most effective one.

###8. Model Training and Evaluation:

* The models were trained on the preprocessed training data (df_train) after handling missing values and encoding categorical features.
* The data was split into training and validation sets to evaluate model performance.
* Accuracy was used as the primary metric to compare the models.

###9. Model Comparison:

* The accuracy scores of each model on the validation set were stored in the results dictionary.
* The model with the highest accuracy was selected as the best model.

###10. Prediction on Test Data:

* The best model was then used to make predictions on the preprocessed test data (df_test).
* The predicted values were stored in a new column in df_test.

###11. Exploratory Data Analysis (EDA)

* The EDA revealed key insights into the dataset, including data types, missing values, potential biases, and outliers.
* Data visualizations helped identify patterns and potential issues.
* Steps were taken to handle missing values and outliers, and categorical features were encoded.

###12. Models

* Four classification models (Logistic Regression, Random Forest,KNN, Gradient Boosting) were chosen for this task.
* The models were trained and evaluated on the preprocessed training data.
* Accuracy was used as the primary evaluation metric.
* The best model was selected based on its performance on the validation set.
* Predictions were made on the preprocessed test data using the best model.

#Saving the Trained Model
"""

import joblib

# Save the trained model
joblib.dump(best_model, "best_model.pkl")

# Save the encoder (if you used One-Hot Encoding)
joblib.dump(encoder, "encoder.pkl")

# Commenting out or removing the lines related to scaler as it is not used in this notebook
# Save the scaler (if you used feature scaling)
# joblib.dump(scaler, "scaler.pkl")

print("âœ… Model and encoder saved successfully!")  # Updated message

"""#Create Your Streamlit App"""

!pip install streamlit

import streamlit as st
import pandas as pd
import numpy as np
import joblib  # To load the saved model

# Load trained model and preprocessing objects
model = joblib.load("best_model.pkl")  # Load the best trained model
encoder = joblib.load("encoder.pkl")  # Load the categorical encoder

# Streamlit UI
st.title("ðŸ” Employee Retention Prediction App")

# Sidebar inputs for user details
st.sidebar.header("Enter Employee Details")

city_development_index = st.sidebar.slider("City Development Index", 0.0, 1.0, 0.5)
training_hours = st.sidebar.number_input("Training Hours", min_value=0, max_value=500, value=50)
company_size = st.sidebar.selectbox("Company Size", ["<10", "10-50", "50-100", "100-500", "500-1000", "1000-5000", "5000-10000", "10000+"])
company_type = st.sidebar.selectbox("Company Type", ["Pvt Ltd", "Funded Startup", "Public Sector", "NGO", "Other"])
education_level = st.sidebar.selectbox("Education Level", ["Primary School", "High School", "Graduate", "Masters", "PhD"])
experience = st.sidebar.slider("Total Experience (Years)", 0, 30, 5)

# Define all features needed for prediction
categorical_features_for_encoding = ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level',
                                      'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']

# Create user input DataFrame with placeholder values for categorical features
user_input = pd.DataFrame([[city_development_index, training_hours,
                           # Placeholder values for categorical features
                           'Other', 'Male', 'Has relevent experience', 'no_enrollment', 'Graduate',
                           'STEM', str(experience), company_size, company_type, '1']],
                          columns=['city_development_index', 'training_hours'] + categorical_features_for_encoding)

# Select categorical features for encoding
user_input_categorical = user_input[categorical_features_for_encoding]

# Encode categorical features using the saved encoder
user_input_encoded = encoder.transform(user_input_categorical)

# Get encoded feature names
encoded_feature_names = encoder.get_feature_names_out(categorical_features_for_encoding)

# Create DataFrame for encoded features
user_input_encoded_df = pd.DataFrame(user_input_encoded, columns=encoded_feature_names, index=user_input.index)

# Concatenate numerical and encoded features
numerical_features_for_prediction = ["city_development_index", "training_hours"]
user_input_final = pd.concat([user_input[numerical_features_for_prediction], user_input_encoded_df], axis=1)

# Ensure test set has the same features and order as X_train
user_input_final = user_input_final.reindex(columns=X_train.columns, fill_value=0)

# Make Prediction
if st.sidebar.button("Predict"):
    prediction = model.predict(user_input_final)

    if prediction[0] == 1:
        st.error("ðŸš¨ The employee is likely to leave!")
    else:
        st.success("âœ… The employee is likely to stay.")

